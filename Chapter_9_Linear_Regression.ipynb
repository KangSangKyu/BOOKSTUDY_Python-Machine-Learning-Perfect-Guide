{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 9. Linear Regression.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyONWyTY4rvrqDwWg/O+SJgM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KangSangKyu/BOOKSTUDY_Python-Machine-Learning-Perfect-Guide/blob/main/Chapter_9_Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVwJzwFOeWZG"
      },
      "source": [
        "# 선형회귀"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxDmbGOLKFxS"
      },
      "source": [
        "# 경사하강법 \n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "# y = 4X + 6 을 근사(W1=4, W0=6) 임의의 값은 노이즈를 위해 만듦 \n",
        "\n",
        "X = 2 * np.random.rand(100,1)\n",
        "y = 6 + 4 * X + np.random.randn(100,1)\n",
        "\n",
        "# X, y 데이터 세트 산점도로 시각화 \n",
        "\n",
        "plt.scatter(X,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQ2Sv7t3KFzk"
      },
      "source": [
        "# w1과 w0를 업데이트 할 w1_update, w0_update를 반환 \n",
        "\n",
        "def get_weight_updates(w1, w0, X, y, learning_rate=0.01):\n",
        "    \n",
        "    N = len(y)\n",
        "    \n",
        "    # 먼저 w1_update, w0_update를 각각 w1, w0의 shape와 동일한 크기를 가진 0 값으로 초기화 \n",
        "    w1_update = np.zeros_like(w1)\n",
        "    w0_update = np.zeros_like(w0)\n",
        "    \n",
        "    # 예측 배열 계산하고 예측과 실제 값의 차이 계산 \n",
        "    y_pred = np.dot(X, w1.T) + w0\n",
        "    diff = y - y_pred\n",
        "    \n",
        "    # wo_update를 dot 행렬 연산으로 구하기 위해 모두 1값을 가진 행렬 생성 \n",
        "    w0_factors = np.ones((N,1))\n",
        "    \n",
        "    # w1과 w0을 업데이트할 w1_update와 w0_update 계산 \n",
        "    w1_update = -(2/N)*learning_rate*(np.dot(X.T, diff))\n",
        "    w0_update = -(2/N)*learning_rate*(np.dot(w0_factors.T, diff))\n",
        "    \n",
        "    return w1_update, w0_update"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VllofcBIKF2B"
      },
      "source": [
        "# 입력 인자 iters로 주어진 횟수만큼 반복적으로 w1과 w0를 업데이트 적용함 \n",
        "\n",
        "def gradient_descent_steps(X, y, iters=10000):\n",
        "    \n",
        "    # w0와 w1을 모두 0으로 초기화 \n",
        "    w0 = np.zeros((1,1))\n",
        "    w1 = np.zeros((1,1))\n",
        "    \n",
        "    # 인자로 주어진 iters 만큼 반복적으로 get_weight_updates() 호출해 w1, w0 업데이트 수행 \n",
        "    \n",
        "    for ind in range(iters):\n",
        "        w1_update, w0_update = get_weight_updates(w1, w0, X, y, learning_rate=0.01)\n",
        "        w1 = w1 - w1_update\n",
        "        w0 = w0 - w0_update\n",
        "        \n",
        "    return w1, w0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSGDJ26KKOAY"
      },
      "source": [
        "#  예측값과 실제값의 RSS 차이를 계산하는 함수 \n",
        "\n",
        "def get_cost(y, y_pred):\n",
        "    N = len(y)\n",
        "    cost = np.sum(np.square(y - y_pred)) / N\n",
        "    return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bl-5SoriKPSh"
      },
      "source": [
        "w1, w0 = gradient_descent_steps(X, y, iters=1000)\n",
        "print('w1:{0:.3f} w0:{1:.3f}'.format(w1[0,0], w0[0,0]))\n",
        "\n",
        "y_pred = w1[0,0] * X + w0\n",
        "print('Gradient Descent Total Cost:{0:.4f}'.format(get_cost(y, y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A25HKq4lKQmD"
      },
      "source": [
        "plt.scatter(X,y)\n",
        "plt.plot(X, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRqZtWwDKR8s"
      },
      "source": [
        "# 확률적 경사하강법(미니배치)를 이용한 비용함수 최적화 \n",
        "\n",
        "def stochastic_gradient_descent_stpes(X, y, batch_size=10, iters=1000):\n",
        "    w0 = np.zeros((1,1))\n",
        "    w1 = np.zeros((1,1))\n",
        "    prev_cost = 100000\n",
        "    iter_index = 0\n",
        "    \n",
        "    for ind in range(iters):\n",
        "        np.random.seed(ind)\n",
        "        \n",
        "        # 전체 X, y 데이터에서 랜덤하게 batch_size 만큼 데이터를 추출해 sample_X, sample_y로 저장 \n",
        "        stochastic_random_index = np.random.permutation(X.shape[0])\n",
        "        sample_X = X[stochastic_random_index[0:batch_size]]\n",
        "        sample_y = y[stochastic_random_index[0:batch_size]]\n",
        "        \n",
        "        # 랜덤하게 batch_size만큼 추출된 데이터 기반으로 w1_update, w0_update 계산 후 업데이트 \n",
        "        w1_update, w0_update = get_weight_updates(w1, w0, sample_X, sample_y, learning_rate=0.01)\n",
        "        w1 = w1 - w1_update\n",
        "        w0 = w0 - w0_update\n",
        "        \n",
        "    return w1, w0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IduVVMaKTqn"
      },
      "source": [
        "w1, w0 = stochastic_gradient_descent_stpes(X, y, iters=1000)\n",
        "print('w1:', round(w1[0,0],3), 'w0:', round(w0[0,0],3))\n",
        "\n",
        "y_pred = w1[0,0] * X + w0\n",
        "print('Stochastic Gradient Descent Total Cost:{0:.4f}'.format(get_cost(y, y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShpxObeuKU7t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}